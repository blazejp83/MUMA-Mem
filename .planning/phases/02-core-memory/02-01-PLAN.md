---
phase: 02-core-memory
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/llm/provider.ts, src/llm/factory.ts, src/llm/index.ts, src/plugin.ts, src/index.ts]
autonomous: true
---

<objective>
Implement the LLM provider that powers the write pipeline's Extract, Decide, and Evolve steps.

Purpose: Phase 2 write pipeline requires LLM calls for fact extraction, deduplication decisions, and context evolution. Phase 1 only defined the LLMProviderConfig type — this plan implements the actual client.
Output: Working LLM client with JSON mode support, factory function, and plugin lifecycle integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/llm/types.ts
@src/plugin.ts
@src/index.ts
@src/config.ts
@src/embedding/remote.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement OpenAI-compatible LLM client with JSON mode</name>
  <files>src/llm/provider.ts</files>
  <action>
Create LLMProvider class following the same fetch()-based pattern as RemoteEmbeddingProvider in src/embedding/remote.ts.

Interface:
```typescript
export interface LLMProvider {
  generate(prompt: string, options?: GenerateOptions): Promise<string>;
  generateJSON<T>(prompt: string, options?: GenerateOptions): Promise<T>;
  readonly modelName: string;
}

export interface GenerateOptions {
  temperature?: number;
  maxTokens?: number;
  systemPrompt?: string;
}
```

Implementation:
- Constructor takes config from MumaConfig.llm (provider, model, apiKey, baseUrl, temperature, maxTokens)
- generate() sends POST to `${baseUrl}/chat/completions` with messages array (system + user)
- generateJSON<T>() adds `response_format: { type: "json_object" }` to the request body, parses response as JSON
- Default baseUrl: "https://api.openai.com/v1" (same as embedding remote)
- Error handling: 401 (auth), 429 (rate limit), 500+ (server error) with clear error messages prefixed with "[muma-mem]"
- If apiKey is empty/undefined, throw on construction with helpful message about configuring llm.apiKey

Do NOT:
- Add streaming support (not needed for pipeline steps, adds complexity)
- Add retry logic (keep it simple, caller can retry)
- Import any external HTTP libraries (use Node.js built-in fetch)
  </action>
  <verify>npx tsc --noEmit passes with no errors</verify>
  <done>LLMProvider class exported from src/llm/provider.ts with generate() and generateJSON() methods</done>
</task>

<task type="auto">
  <name>Task 2: Factory function + plugin lifecycle integration</name>
  <files>src/llm/factory.ts, src/llm/index.ts, src/plugin.ts, src/index.ts</files>
  <action>
1. Create src/llm/factory.ts:
   - Export `createLLMProvider(config: MumaConfig): LLMProvider`
   - Reads config.llm, constructs and returns LLMProvider instance
   - If config.llm.provider is not set AND config.llm.apiKey is not set, return null (LLM is optional — some users may only use read pipeline)

2. Create src/llm/index.ts barrel:
   - Re-export LLMProvider interface and GenerateOptions from provider.ts
   - Re-export createLLMProvider from factory.ts

3. Update src/plugin.ts:
   - Add module-level `let llmProvider: LLMProvider | null = null;`
   - Add `getLLMProvider(): LLMProvider` accessor (throws if null with "[muma-mem] LLM provider not configured. Set llm.apiKey and llm.model in config.")
   - In gateway_start hook, after store init: call createLLMProvider(config), assign to module state, log model name if available
   - In gateway_stop hook: set llmProvider = null
   - Import createLLMProvider from ./llm/factory.js

4. Update src/index.ts:
   - Re-export `getLLMProvider` from plugin.ts
   - Re-export `type { LLMProvider, GenerateOptions }` from llm/index.ts
  </action>
  <verify>npx tsc --noEmit passes; grep -q "getLLMProvider" src/index.ts</verify>
  <done>Factory creates LLM provider from config, plugin initializes it on gateway_start, getLLMProvider() accessor available for pipeline modules</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npx tsc --noEmit` succeeds
- [ ] src/llm/provider.ts exports LLMProvider class with generate() and generateJSON()
- [ ] src/llm/factory.ts exports createLLMProvider()
- [ ] src/plugin.ts has getLLMProvider() accessor
- [ ] src/index.ts re-exports getLLMProvider and LLMProvider type
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- LLM provider follows same patterns as embedding provider (fetch, error handling, module singletons)
  </success_criteria>

<output>
After completion, create `.planning/phases/02-core-memory/02-01-SUMMARY.md`
</output>
